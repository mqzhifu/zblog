---
title: 分布式系统
date: 2022-09-15 14:59:40
categories: ["分布式"]
tags: ["cap","base","一致性","共识","事务"]
---
概述
--
分布式系统：一组计算机协作的计算。计算机之间：通过网络通信各实现关联性，共同完成某一类任务。

分布式系统解决的问题：
1. 提高单台机器性能(计算量)
2. 提升单台机器存储空间

单台变多台，肯定得通过网络保持通信，连带着就要考虑：传输协议、传输安全
难点：原本一台搞定的时候，现在变成了多台协同完成，那么，如何保持状态的同步？比如是否有顺序性？某一台挂了，其它的节点如何容错？


分析出关键词：
1. 容灾
2. 性能
3. 通信协议

为什么要用分布式？
----
场景：
1. 当web请求量爆增的时候
    1. 单台机器的读写速度
    2. 单点故障
2. 当业务需要大量计算时：把一个大的任务，分解成若干子任务，然后分发到每台机器上去并行计算，最后再将结果合并起来
3. 空间：单台存储肯定有限，把数据集分散到多台机器上存储

感觉概念挺大、挺宏观，应用的场景其实也非常的多：
1. 分布式存储：数据库、文件
2. 分布式缓存：redis memcache
2. 单点故障容灾：LVS haproxy nginx 客户端自研
3. 分布式计算：负载均衡、任务管理系统、队列系统、微服务架构

依然感觉概念好大，牵扯的开源软件很大，牵扯的概念也样很大。而且，互相牵扯。如：分布式存储也包括单点故障容灾和分布式计算。
>这也是为什么学起来难，容易混淆。并且，很多软件都可以自称为分布式的原因

站在开发者(应用层)的角度：可能处处都是分布式，但凡你用了一个中间件/开源软件，可能就支持分布式，所以，这个分布式的概念非常需要学习。


从最简单的角度看：
以MYSQL主从为例，它也算分布式，是两台机器协作，读写分离，解决了性能问题。
如果再在这两台机器上再加一层代理模式，且实现主从自动切换(从库也可以支持写操作)，那么还有单点故障解决的特性

那么，只要是用到2台机器完成某一类任务，就是分布式。软件开发中真的处处都是分布式


CAP理论
----
1998年

上面的概览还挺大的，过于复杂，我们看看能不能抽离出分布式的一些共性，便于落地

注意：不要与数据库中的AICD 混淆，CAP理论中的C更强调的是在分布式系统下，各节点上存储的副本数据的一致性。

一个分布式系统：最重要的3个点:

| 英文 |中文  | 解释 |
| --- | --- | --- |
| consisitency | 一致性 | 所有的节点副本，在同一时间，看到的数据都是完全一致的 |
| availability |可用性  | 在集群中一部分节点故障后，集群的其它节点还能响应客户端的读写请求，但不能保证数据是最新的 |
| Partition tolerance |分区容错性  |  集群内部通信，某台机器出现故障：丢包、延迟，造成内部数据可能不一致，但仍然能够对外提供服务。那么数据就得存于多台机器上|

#### 分区容错性
在理想状态下，其实不需要考虑CAP这个问题，在一些特殊情况下，才有CAP的发展空间

如：一个DB，55开，分成两份数据，各存于两台机器上，用户可任意读写其中一台
1. 假设，此时服务器1挂了，客户端请求这台服务器，是无法得到响应的，也就是无法对该机器上的数据读写操作
2. 可此部分数据是比较重要且核心的，那么间接导致整个集群都不能用了
3. 此时，把整个DB的数据，完全存于两台机器上(不做55拆分了)，那么就不会有这种情况了，现在就变成了

分区容错性:某一个节点挂了，其它节点仍有完整的数据，也可以为client提供服务
>如果单点挂了造成整个集群不可用，那不就成单点机了么......

产生新问题：原本一份数据变成了两(多)份，那么就得有网络同步机制，就会有延迟、丢包的情况，那么就出现了新的问题：数据不一致

#### 一致性
1. 强一致性，也可以理解为同步复制。也称为：原子一致性（Atomic Consistency）、线性一致性（Linearizable Consistency）
    1. client 请求 server1 ，server1在本地更改，再通知serve2 server3一并更改，server2 server3 返回确认消息告诉server1 我OK了，此时server1 再告知client成功
    1. 在这期间client如果还有其它的访问会被阻塞，因为是顺序执行
    2. 在这期间如果其它client也有请求，且要读写该数据，进入阻塞
    2. 如果 server2|server3 响应 server1 慢，会导致 client 阻塞
    3. 如果 server2|server3 挂了，那 client 被永久阻塞了，要么有个定时器超时，要么只能等它两恢复
    4. 缺点严重：性能差、可用性低
1. 弱一致性：也可以理解为异步复制
    1. 流程跟上面一样，只是它不会阻塞
    2. 缺点：可能读出的值是旧的(不准确的)，不是最新的，什么时候能准确？我也不知道....
    3. 优点：性能高、可用性高
    1. 如：域名结点
2. 最终一致性：
    1. 不一致可能是暂时的，最终要一致（不确定“最终”是多久，但至少有这么一个时间值，具体实现的时候可定义一个）
    2. 感觉跟弱一般性差不多呢，但它可能更突出的是：最终肯定是一致的
    1. 如：DNS ,root -> 二给(ddu com net gov ) -> 三级...  一级一级推，如果你的DNS服务器在3级，直接访问，就是旧的IP映射地址。还有GOSSIP(cassandra)
>这种有点像是：时间换结果，如果有变更，给定一个等待值，在这期间别读，过了等待时间读取的最终值，肯定是对的


#### 可用性
一致性又牵扯出：可用性，即上面所说的阻塞，一但用强一致性，必定会引起阻塞，造成可用性降低。
使用弱一致性或最终一致性，是可以提高系统的整体可用性的。


#### CAP的具体使用

首先，必须得有：分区容错性，如果某个节点挂了，导致整个集群挂了，这肯定不行的，基于此观点，我们再举两个例子：
写操作：
1. 服务器1网络出现了问题，客户端请求这台服务器，是无法得到响应的
2. 服务器2检测到挂服务器1了，继续提供服务
3. client请求服务器2写入操作，该写入操作暂时无法同步到服务器1
    1. 保证一致性，给client返回写入失败 或者 进入阻塞模式，直到服务器1恢复正常，能正常把消息同步过去，再返回client成功
    2. 保证可用性，给client返回写入成功。（等server1正常了，后续再通过日志追补数据）

读操作：
1. 服务器1挂了，客户端请求这台服务器，是无法得到响应的
3. client请求服务器2读操作
    1. 保证一致性，给client返回失败 或者 进入阻塞模式，直到服务器1恢复正常，确定数据是一致的，再返回client成功
    2. 保证可用性，给client返回成功(可能数据不一致，不是最新的)

#### 推论
1. CP，保证一致性，牺牲掉可用性。如发生数据不一致，是错误的，所以返回失败|阻塞。关系型 中用的多一些，MYSQL
2. AP，保证可用性，牺牲掉一致性，如发生数据不一致，但依然可以返回(可能数据不是最新)。NOSQL型 用的多一些，redis
>题外话：如果放弃一致性，你返回的数据可能是旧的(错的)，那谁还敢用呢？哈哈
3. CA，保证一致性/可用性，那这就是单机模式了，也就不存在分布式了......

CAP只是一种理论，并没有具体的实现细节。具体的实现可根据自身的场景做定制化实现。
另外，它更像是一个反向理论，告诉你：不能实现什么，但并不是：完全不能实现，或者说只能实现CP | AP 模式，也可以 CP+High Availability模式




强一致性：
1. 同步执行逻辑
2. paxos
3. raft(multi paxos)
4. zab(multi paxos)


BASE理论
----
三个短语的缩写：Basically Available(基本可用) Soft State(软状态) Eventually Consistent(最终一致性)
定义：基本可用的AP模式的分布式架构
>是对 CAP 中一致性和可用性权衡的结果，从<Eventually Consistent>也能看出，它是对一致性的妥协，反过来看，它是：AP模式的加强版

它选择的不是强一致性，而是：最终一致性，同时又加入 Soft State术语。

#### 基本可用
当系统部分机器出现故障的时候，保证核心功能可用，非核心的功能可以不可用：
1. 延迟响应：比如：请求过大，可以先把所有请求先放到队列中，再慢慢处理，最终返回正确的结果
2. 功能损失：比如：一次购买流程，需要下单 支付 发送短信/邮件通知，下单 支付要一定可用，而消息通知可以不可用
3. 体验降级：把高清图片，看成标清
4. 过载保护：请求过大，直接拒绝

总之，当系统进入不稳定期，或预估可能将进入不稳定期，牺牲掉一些功能|性能，来保证系统的核心功能正常，提高可用性

#### 软状态
相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种 “硬状态”。或者说，数据就没有状态，只要数据存在就证明它是对的。

软状态：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。
翻译过来就是：一份数据，在不同的时间段读，可能是不一样，就是数据可能会一直在变，但是，最终数据的值是不变的。


#### 最终一致性
所有数据的副本，在经过一段时间同步后，最终能够达到一致的结果
>划重点：一段时间后一致/数据会产生延迟

当给一个数据，被添加上中间状态后，再给它设定一个超时时间，最终在这个时间段之后，它的中间状态会被删除，数据的值也就是最终状态了（是一致的）


简单的整体思路：A要修改一个数据，那么给该数据加上一个临时状态，然后开始整个事务的处理，在这期间可能A在更改数据值，也可能A调用B，B也在更新，反正，数据就是一直在变，走到整个事务均执行成功，把这个临时状态改成一个最终状态。

那么如何实现最终一致性呢？你首先要知道它以什么为准，因为这是实现最终一致性的关键。一般来说，在实际工程实践中有这样几种方式：

以最新写入的数据为准，比如 AP 模型的 KV 存储采用的就是这种方式；
以第一次写入的数据为准，如果你不希望存储的数据被更改，可以以它为准；
那实现最终一致性的具体方式是什么呢？常用的有这样几种。

读时修复：在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据；
写时修复：在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败就将数据缓存下来，然后定时重传，修复数据的不一致性；
异步修复：这个是最常用的方式，通过定时检测副本数据的一致性，并修复；


CAP 与 BASE 区别
----
CAP更倾向于纯理论，而BASE还算是能有点落地，考虑一些中间值的处理，像：妥协强一致性，增加可用性。
>感觉base像是对CAP的延伸/补充

不过，严格说两个都是理论，具体还是得看哪些开源软件实现了，怎么实现了，改了多少


可靠性
----
可靠性使用平均无故障时间（Mean Time Between Failure，MTBF）来度量
可维护性使用平均可修复时间（Mean Time To Repair，MTTR）来度量。


分布式共识算法
----

先说一下，强一致性，需要所有节点都达成了共识，才证明本次操作是OK的。
最简单的共识就是：A 给 B C 发送同步消息，然后阻塞等待，过了一会，B C 同时返回消息给A：可以了我们都同步好了。

如果牺牲一部分共识算法，就可以换取一定的可用性，那么，共识算法就是核心。

共识算法：也可以理解为是  一致性+可用性的载体，如果有较好的共识算法，也就算是实现了分布式系统


#### 主流开源的分布式共识算法：
|  |  |
| --- | --- |
| 2PC | 二阶提交，将一个事务分成两部分，第一部分用于确认其它节点是否正常，第二部分再提交 |
| 3PC | 三阶 提交，将一个事务分成三部分，第一部分用于确认其它节点是否正常，第二部分再提交，最后一部分再确认 |
| BASIC PAXOS | 基于消息传递的，一致性算法 |
| MULTI PAXOS | 对PAXOS算法的升级，提出了leader模式 |
| FAST PAXOS | 对PAXOS算法的升级 |
| RAFT | 基于multi PAXOS算法的升级，也是leader选举模式 ,etcd consul使用|
| ZAB |  对 FAST PAXOS算法的升级，也是leader选举模式 ,zookeeper使用|


>以上各种算法我都写了相关文档，有兴趣的可以找我要

#### 2PC 3PC 对比：
2PC 3PC 好像是DB中用的比较多，强一致性，对数据的一致性要求非常高。
所以，因为它大概率牺牲掉可用性，并发效果太差。其它场景使用不太高，或者其它场景使用也是改进版


#### PAXOS 分析
BASIC PAXOS 、 MULTI PAXOS 、 FAST PAXOS 宽松看都是PAXOS理论，是提出最早的论文，更倾向理论派+学习作用，其它的协议都是基于此协议上进行改造升级的，有那么点：抛砖引玉的意思


#### RAFT ZAB 对比：

>目前个人觉得，这两种是比较主流使用的

ZAB 属于 zookeeper 实现，zookeeper又是java编写并捐给了apache，再加上JAVA系各大厂商使用与推广，它应该算是非常稳定且早期的产品。或者干脆说：之前它是属于封神的状态

zab更倾向于CP，对可用性牺牲略大，而raft更强调的是大集群高并发、高性能的前提下的，慢慢的raft就领先了

随着大数据、微服务的崛起的，机器越来越多，复杂度越来越高，zk就有点落伍

所以结论：raft才是未来


基于base理论，分布式衍生出两个方向
----
1. 开源的分布式软件/中间件
2. 分布式事务



开源的分布式软件/中间件
----
基于zab raft协议，主流的开源软件：zookeeper etcd consul
还有一些软件是基于这3个软件的基础之上做开发的：(仅列举些常见)
1. zk: dubbo kafka(新版本已使用raft) seata hadoop
2. consul: prometheus apisix
3. etcd:k8s

>肯定还是 zk 更多，依托JAVA开发且问世较早的优势。而且，虽然它现在处于落伍阶段，但是如果追求稳定性，它肯定是首选 ，因为它经历了时间的考验

分布式中间件的主要功能：
1. KV数据、元数据存储、状态存储
>就是get/set数据，这个是最核心的，下面两条有点附加功能的意思
2. watch:对数据进行监听
3. 锁:可以申请锁管理


主要应用的场景：
1. 服务发现/注册中心/命名服务
2. 配置中心
3. 容灾，某台挂了，并不影响其它机器，整个集群会选新leader并依赖可提供服务
4. 负载均衡(读写分离、多副本一致性可提高读的效率，使用DNS可实现负载均衡)
4. 集群管理，分布式场景下、各机器的协调

#### zk etcd consul 对比

zk 最早，etcd consul 是踩着巨人肩膀上而生，结合了ZK的各种优先，并去掉缺点，所以，zk的可比性真的不太高。那主要对比的就是etcd consul了

etcd火，个人感觉跟google大力推golang 有关系 ，K8S使用的就是etcd，那它火了也是正常。
感觉etcd特别的小巧，没那么大，又结合了zk的一些思想，很小巧

consul感觉比较大，但是它的各种配套就是比较齐全，尤其像UI管理、支持HTTP API、健康检测、DNS轮询、多机房等

反正，个人现在是不推荐zk了。不是它不好，是因为它太老了。
首选，ETCD，毕竟用公布式框架的都是集成，那么它更应该回归到一个中间件的本性，太复杂了也没必要
最后，是consul

分布式锁
---
跨请求/跨进程/跨服务器 -> 请求共享资源(数据库、文件、全局变量)

具体什么是锁？为什么使用锁？锁的机制及分类介绍，请参考另一篇文章(有兴趣的找我要)，这里主要是讲：分布式，而不是锁机制

分布式加锁最主要的是：可以跨机器
1. 如果是跨机器，锁这个标识，不管载体是文件还是内存，肯定得存于某个地方(服务器)，最好是存于3方，而不是某个应用的服务器上。
2. 如果存于某台机器上，那么肯定会有单点故障问题，那最好能分布式存储，保证安全

结合以上两点：可存储锁(标识数据)、非单点存储有容灾功能、非单点存储还要保证性能、非单点存储还要保证数据的一致性

好像以上均是分布式性软件(中间件)的所有特性，它天然就间接等于支持了分布式锁，所以：
1. zookeeper
2. etcd
3. consul
4. redis

这些比较经典的分布式开源软件框架，就间接可以实现锁的功能，有的直接就带：zk consul ，有些的可能得间接实现下etcd



zk etcd consul
--------
它们3个是比较主流的分布式框架中间件，我看了一些文档，做了些调研，写了一些文档，但分析的都还不够细致，时间+精力不够，我空了，把文档都补全了再分享出来吧。

分布式事务
----
又是一个大课题，不在这里写了，参考另一篇文章吧，有兴趣的找我要

总结
---
分布式之所以会火，并有人关注，完全是因为互联网的火爆，这其中尤其是：大数据、微服务、区块链的崛起，加速了分布式的火爆。
分布式虽然很大，但其实只要找到一个核心去学习，也不难，比如：etcd zk consul。
现实业务的场景中，其实也不需要学的多么的精，因为有一堆科学家提供理论基础知识，各大公司或牛人会实现这些理论，并开发出开源的框架，对于大中型公司的程序员，其实只要会用即可。

个人感觉这种东西学了，使用场景不多，如果对技术没太大兴趣的，完全可以不用研究，对技术感兴趣的可以深入研究下。但最终，感觉还是为了面试的时候使用，可要更多的工资罢了。毕竟，能有几家公司会自行开发分布式，或者有那么大的数据量让你去调优呢？