---
title: zab
date: 2022-09-15 15:10:20
categories: ["分布式"]
tags:
---
zookeeper atomic broadcast:zookeeper原子广播协议
>fast paxos协议的简化版本


最终一致性
强/单leader模式


leader-follower 写交互：2次(2阶段提交的模式)
client-leader 写交互：1次



单leader优点
----

1. 原子操作：递增、写操作
2. 原子广播：由一台机器向其它机器广播
3. 消息的顺序性(队列)

使用zk的中间件
----
dubbo:注册中心 配置中心
spring cloud 注册中心
solr 管理集群
kafka 管理集群
curator 分布式锁
tinyld 分布式ID
seata 配置中心
大数据库领域

内部结构
-----

1. dataTree-->真实的数据，在内存中。读写最终的数据是它。
2. 投票箱-->好像是个数组，[ ( 2 , 2, 100 ) ] ,[ ( 投票者myid, 投给谁 myid, 本机 zxid ) ]
3. 日志文件--》zxid ,事务ID，自增连续ID
    1. 高32位-epoch -->任期
    2. 低32位-counter -->计算数
4. myid -->配置文件中写死的
5. 队列，广播发送消息的时候，内部维持一个队列

每台zk节点，都会有一个日志文件
每来一个请求，会记录到日志文件中，当时zxid+1

>zxid，如：100000001  ,100000002......200000001,200000002

角色
----
leader 通过选举，在集群中选出一台节点当leader，失败重启后，可参加选举
follower 跟随leader的脚步，用户的读操作直接在本节点操作(也可设置sync参数)，写操作则需要转到leader执行，可参加选举
observer 观察者/备用机，依然能提供读操作直接在本节点操作，写操作则需要转到leader执行，不参考选举，不参加第一阶段预准备


启动/初始化
---
1. 读取 FileTxnSnapLog
2. 构建 ZKDatabase
3. PlayBackListener
4. 将硬盘里的日志文件恢复到内存中,Zookeeper会加载最新的至多100个快照文件
5. 检查 ZXID epoch


启动/数据同步
----
1. 好像所有服务器启动后，都会开启一个 Learner 进程
2. leader选举成功后，每个follower的 Learner 进程会向leader进行注册
3. 当所有正常的机器注册成功 Learner 后，开始数据同步
4. Learner 服务器会发送给Leader服务器一个Ack_Epoch_lastZxid 数据包
5. Leader接收，并解出 EPOCH
6. 通过两端的epoch+zxiid，做出下面4种处理：
    1. 直接差异化同步(diff同步)，简单说：对端的日志完全落后于我，把leader上的日志整理下打包发给对端，让它补全即可
    2. 先回滚再差异化同步，对端的机器上有未提交的日志。如：旧leader日志已经写入等待ACK的时候挂了，另外机器重新选举成功，并正常又接收新的消息了，旧leader启动后，那么就会出现：旧leader未提交的那条日志，新leader中没有，leader找出具体位置后，让那条未提交的数据回滚/删除，然后再开始对落后的数据进行同步
    3. 回滚同步，发现 Learner 的 lastZxid 比自己的 maxCommittedLog 还大，那直接让Learner回滚到maxCommittedLog值即可。这种情况：旧leader有一条数据未同步就挂了，另外3台机器重选 ，可能有一台日志比较落后，但是也成功当选了新leader，那就以新leader为最准的，把旧leader未同步的数据直接删除
    3. 全恢复，如果 Learner 的 lastZxid 比自己的 minCommittedLog 还小，直接把新leader覆盖过去即可

>启动、初始化、数据同步/修复阶段，肯定是无法对外部提供服务的，选举期间也是不能的，必须保证各节点的数据全部正确后，才可以。感觉ZK还是挺保守的。


选举
---
1. 数据恢复
1. 每台机器启动后，会找leader，或被leader找
2. 如果找不到leader或也没有leader找自己
3. 如果自己不是observer角色
4. 开启选举，从配置配置文件中找到其它机器的ip+port
5. 与其它机器建立socket连接
6. 如果在这期间收到消息：当前集群有leader，你可能只是与它短暂未连接上，先退回到follower状态
6. 第一次投票，默认肯定是要投自己的
7. 将zxid + myid 打包，广播给其它的机器
8. 更新本机的投票箱，将自己的选票存进去
9. 如果接收到其它机器的选票
    1. zxid 比自己的小，给自己投票，并存到箱子中(不重复投)，然后广播给其它follower
    2. zxid 相等，再比较 myid
        1. myid 比自己大，将自己选自己的选举删除，将对方的选票保存，同时自己再投对方一票，保存本地，再广播该投票
        3. myid 比自己的小，给自己投票，并存到箱子中(不重复投)，然后广播给其它follower
    3. zxid 比自己大，将之前的选票(自己选自己的)删除，将对方的选票保存，同时自己再投对方一票，保存本地，再广播该投票

>每台机器来来回回：会收到多张选票，同时触发自己的被动机制，再发出新的选票(比对zxid+myid)。最终每个节点的选票箱其实结果是一样的
>比对zxid+myid过程中，发现对方更大就修改自己的投票，这个算是：选举谦让制度，非抢占式
10. 各节点不停的统计选票，只有过半，即认为自己是leader
11. 新leader，同步/广播给其它选举节点，告知自己是leader
12. 其它机器接收到leader发过来的广播消息，自己将角色切换成follower


>两台机器交换选票肯定是通过网络socket，两台机器如果一起启动，互相建立socket连接，如果是myid 小的连myid大的，会被myid大的给拒掉

>选举过程中：肯定是无法处理client请求的。

如果：现在集群是5台机器
1. 如果先启动1 2 ，另外3台不启动，能选出leader么？肯定不能，两台机器的选票是不可能过半的
2. 如果全部正常，leader为2，此时 3 4 5 挂了，还能提供服务么？肯定不能，因为消息的ack永远不可能过半。leader发现一半挂了，它会由leader转换成follower，重新选举


读写操作
---
所有角色均能接受client读写操作，读操作在本地均可处理，写操作follower和observer要转交给leader处理

leader接收写请求：
1. 判断前一个预提案是否生效（过半节点回复了上条消息的ack）
    1. 如果未生效，阻塞，直到上一个提案OK了
    2. 如果已生效，继续下面
>这就保证了时序性/顺序性
2. 生成日志数据，zxid+1
2. 将日志持久化
3. 预提交
4. 阻塞，等待follower ack
5. 提交,更新dataTree
6. 将日志异步广播到follower
7. 将日志异步广播到observer
8. 返回client成功


特殊的情况1：步骤4，如果未收到过半ACK？两种情况：
1. 设一个定时器，超时,回滚，返回client失败
2. 或者节点之间有心跳，检查出有一半节点down机或一半节点网络错误，开启重新选举(回滚，返回client失败)
3. leader挂了(网络波动，短时间内不能恢复)，重启/恢复后，该条数据被删除
4. 网络波动，其它节点开启重选，机器马上恢复参与选举，因为zxid大，还会继续成为leader，选不上，该条数据删除

特殊的情况2：步骤4，leader本地已提交，在通知其它follower提交时挂了
1. 如果新leader是已收到的那条消息情况，当然新leader，那么数据正常
2. 如果新leader是未收到的那条消息情况，当然新leader，该数据就丢失了

>因为二阶段提交的核心思想就是，第一阶段来确认其它机器是否这正常，发生上面的错误的概率极低，就算发生了，但好像raft zab对这种情况好像都没做明确说明，不知道具体原因。
>如果zab是顺序存储(阻塞模式)，最多只能丢失一条，貌似还好。



observer及IO分析
----
这里有个问题：机器越多，读操作更快。但是写会慢？因为，需要同步半数的机器，等待ACK。
observer：
1. leader 最终将已提交的日志，再异步转发给observer
2. 不参与选举
3. 提高写操作，leader 同步日志，不需要等待 observer的ack
4. 增加读操作

>如果机器变多后，把其角色定义成observer角色，可以提高读的速度，同时减少写的时候有过多的rsync

读写分离后分析：

1. 选举的速度肯定是由follower决定(网络原因先不算,旧leader要那么直接挂了不参选，要么已经转变成了follower)
2. 读的速度由：follower+observer 数量决定
3. 写的速度由：leader + 同步follower数量决定

脑裂分析
---
会有脑裂么？
1. 如果是6台，挂了3台，就不会有leader，未过半
2. 如果是5台，挂了2台，3台那部分是正常，另外2台如果是leader会放弃重新选，但永远选不出来leader

如果机器总数是偶数，会是什么样？如果是6台，挂了3台，另外3台是肯定选不出leader


所以整体看，还是奇数台机器更优。
>不过这里要注意：忽略掉observer



watch
-----

锁
----

数据文件树
-----

